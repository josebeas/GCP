# Creating tables and loading data
## GCP BigQuery
When you load data into BigQuery, you can:
* load data into a new table or partition. 
* append data to an existing table or partition.
* overwrite a table or partition. 

There is no need to create an empty table before loading data into it. Tables can be created and loaded at the same time.

When you loading data into BigQuery, user can supply the table or partition schema[^1] or in case of supported data formats, user can use schema auto-detection.

Useful links: https://cloud.google.com/bigquery/docs/

## AWS RedShift
Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes, using AWS-designed hardware and machine learning to deliver the best price performance at any scale. 

When loading data into AWS Redshift, using COPY command is recommended for large data sets.

The COPY command appends the new input data to any existing rows in the table.

The maximum size of a single input row from any source is 4 MB.

***
### Create an empty table with schema defined
#### BigQuery using `bq mk` command 
```
bq mk \
--table \
--expiration integer \
--description description \
--label key_1:value_1 \
--label key_2:value_2 \
project_id:dataset.table \
schema
```

with optional parameters as:

* --expiration
* --description
* --time_partitioning_type
* --destination_kms_key
* --label

Example:
```
  bq mk \
  -t \
  --expiration 3600 \
  --description "This is my table description" \
  --label organization:development \
  mydataset.mytable \
  fieldA:STRING,fieldB:FLOAT,fieldC:STRING
```

#### Redshift using AWS cli `aws redshift-data execute-statement` command

Create cluster
```
aws redshift create-cluster --node-type dc2.large --number-of-nodes 2 --master-username adminuser --master-user-password TopSecret1 --cluster-identifier jbeascluster --region us-east-2 --db-name jbeas_database 
```

create bucket
```
aws s3api create-bucket --bucket jbeasbucket --region us-east-2 --create-bucket-configuration  LocationConstraint=us-east-2
```

copy from other bucket to my bucket
```
aws s3 cp s3://de-bootcamp-2021/all_data.csv s3://jbeasbucket
```

Create table
```
aws redshift-data execute-statement \
  --database database \
  --db-user user \
  --cluster-identifier cluster \
  --region aws-region \
  --sql "CREATE TABLE my_table (fieldA varchar (255),fieldB varchar (255), fieldC varchar (255));"

```

Example:
```
aws redshift-data execute-statement \
  --database jbeas_database \
  --db-user adminuser \
  --cluster-identifier jbeascluster \
  --region us-east-2 \
  --sql "CREATE TABLE my_table (producto varchar (255),presentacion varchar (255), marca varchar (255), categoria varchar (255), catalogo varchar (255), precio varchar (255),fechaRegistro varchar (255), cadenaComercial varchar (255),giro varchar (255),nombreComercial varchar (255), direccion varchar (255), estado varchar (255), municipio varchar (255), latitud varchar (255), longitud varchar (255));"
```

Useful links: https://docs.aws.amazon.com/cli/latest/reference/redshift-data/index.html

***
## Create a table from existing sources
In the case we plan to create a table from existing data. Data must be on following formats [^2] and available on GCS/S3 or in a local file:


### Load from CSV file into GCP BigQuery
```
    bq --location=location load \
    --source_format=CSV \
    dataset.table \
    path_to_source \
    schema
```

Example:
```
    bq load \
    --max_bad_records=1000\
    --autodetect \
    --source_format=CSV \
    de_app_jbeas2_dataste.mytable \
    gs://de-appreticeship-jbeas/all_data.csv
```

***
### Load from CSV file into AWS RedShift
```
aws redshift-data execute-statement \
      --database database \
      --db-user user \
      --cluster-identifier cluster \
      --region aws-region \
      --sql "COPY table_name (cols ... ) \
          FROM 's3://<your-bucket-name>/folder/file_name.csv' \
          credentials 'credentials' \
          CSV;" 
```

Example:
```
    aws redshift-data execute-statement \
      --database jbeas_database \
      --db-user adminuser \
      --cluster-identifier jbeascluster \
      --region us-east-2 \
      --sql "COPY my_table (producto,presentacion, marca, categoria, catalogo, precio,fechaRegistro, cadenaComercial, giro, nombreComercial, direccion, estado, municipio, latitud, longitud) \
      FROM 's3://jbeasbucket/all_data.csv' \
      CREDENTIALS 'aws_access_key_id=AKIAT5VT5EUTOAD6KHUX;aws_
      _access_key=x9do8+' \
      DELIMITER ',' \
      IGNOREHEADER 1 \
      MAXERROR as 250 \
      CSV;"
```

***
### Load from JSON into GCP BigQuery
```
    bq --location=LOCATION load \
    --source_format=JSON \
    DATASET.TABLE \
    PATH_TO_SOURCE \
    SCHEMA
```

Example:
```
    bq load \
    --source_format=NEWLINE_DELIMITED_JSON \
    mydataset.mytable \
    gs://mybucket/mydata.json \
    ./myschema
```

Example of ingestion time partitioned table [^1]:
```
bq load \
    --source_format=NEWLINE_DELIMITED_JSON \
    --time_partitioning_type=DAY \
    mydataset.mytable \
    gs://mybucket/mydata.json \
    ./myschema
```
***
### Load from JSON into AWS RedShift
```
    aws redshift-data execute-statement \
      --database database \
      --db-user user \
      --cluster-identifier cluster \
      --region aws-region \
      --sql "copy table \
        from 'location' \
        credentials 'credentials' \
        region 'aws-region' \
        json 'auto';"
```

Example:
```
    aws redshift-data execute-statement \
      --database my_database \
      --db-user adminuser \
      --cluster-identifier mycluster \
      --region us-east-2 \
      --sql "copy table \
        from 's3://project/table.json' \
        credentials 'aws_access_key_id=ACCESS_KEY;aws_secret_access_key=SECRET_ACCESS_KEY' \
        region 'eu-west-3' \
        json 'auto';"
```
***
### Load from ORC file into GCP BigQuery
```
    bq --location=location load \
    --source_format=ORC \
    dataset.table \
    path_to_source
```

Example:
```
    bq load \
    --source_format=ORC \
    mydataset.mytable \
    gs://mybucket/mydata.orc
```
***
### Load from ORC file into AWS RedShift
```
    aws redshift-data execute-statement \
      --database database \
      --db-user user \
      --cluster-identifier cluster \
      --region aws-region \
      --sql "COPY table \
            FROM 'location' \
            credentials 'aws_access_key_id=ACCESS_KEY;aws_secret_access_key=SECRET_ACCESS_KEY' \
            FORMAT AS ORC;"
```

Example:
```
    aws redshift-data execute-statement \
      --database my_database \
      --db-user adminuser \
      --cluster-identifier mycluster \
      --region us-east-2 \
      --sql "COPY table \
            FROM 's3://mybucket/data/table/orc/' \
            credentials 'aws_access_key_id=ACCESS_KEY;aws_secret_access_key=SECRET_ACCESS_KEY' \
            FORMAT AS ORC;"
```
***
### Load using AVRO format into GCP BigQuery
```
    bq --location=location load \
    --source_format=AVRO \
    dataset.table \
    path_to_source
```

Example:
```
    bq load \
    --source_format=AVRO \
    mydataset.mytable \
    gs://mybucket/mydata.avro
```
***
### Load using AVRO format into AWS RedShift
```
aws redshift-data execute-statement \
      --database database \
      --db-user user \
      --cluster-identifier cluster \
      --region aws-region \
      --sql "copy category \
        from 'location' \
        credentials 'aws_access_key_id=ACCESS_KEY;aws_secret_access_key=SECRET_ACCESS_KEY' \
        format as avro 'auto';"
```

Example:
```
aws redshift-data execute-statement \
      --database my_database \
      --db-user adminuser \
      --cluster-identifier mycluster \
      --region us-east-2 \
      --sql "copy category \
        from 's3://mybucket/category_auto.avro' \
        credentials 'aws_access_key_id=ACCESS_KEY;aws_secret_access_key=SECRET_ACCESS_KEY' \
        format as avro 'auto';"
```
***
### Load from PARQUET file into GCP BigQuery
```
    bq --location=LOCATION load \
    --source_format=PARQUET \
    DATASET.TABLE \
    PATH_TO_SOURCE
```

Example:
```
    bq load \
    --source_format=PARQUET \
    mydataset.mytable \
    gs://mybucket/mydata.parquet
```
***
### Load from PARQUET file into AWS RedShift
```
aws redshift-data execute-statement \
      --database database \
      --db-user user \
      --cluster-identifier cluster \
      --region aws-region \
      --sql "COPY listing \
        FROM 'location' \
        credentials 'aws_access_key_id=ACCESS_KEY;aws_secret_access_key=SECRET_ACCESS_KEY' \
        FORMAT AS PARQUET;"
```

Example:
```
aws redshift-data execute-statement \
      --database my_database \
      --db-user adminuser \
      --cluster-identifier mycluster \
      --region us-east-2 \
      --sql "COPY listing \
        FROM 's3://mybucket/data/listings/parquet/' \
        credentials 'aws_access_key_id=ACCESS_KEY;aws_secret_access_key=SECRET_ACCESS_KEY' \
        FORMAT AS PARQUET;"
```


[^1]: Partitions helps to reduce amount of processed data, improves query speed and minimizes cost. Each service offers different partitions strategies, select the one that fits most your project needs.

[^2]: Most commonly used formats while exporting data are pre-defined, see specification datails before choosing between formats.


GCP

# Processed bytes

## creating Table

```
    bq load --autodetect \
      --source_format=CSV \
      mydataset_jbeas.testtable \
      gs://de-bootcamp-2021/all_data.csv
```
will output an error since some fields like precio and fechaRegistro have some tricky values like null, malformed or unkonwn values.

In this case we will have two options, enable max_bad_records config flag and set it to a descent amount
```
    bq load --autodetect \
      --max_bad_records=100 \
      --source_format=CSV \
      mydataset_jbeas.testtable \
      gs://de-bootcamp-2021/all_data.csv
```
will ingest 17.85 GB of data into BQ table `testtable`

Or ...

make data types more flexible, this may reduce precision or limit the ability to use functions like sum, avg since all fields are being handled as strings
.
```
    bq load \
      --source_format=CSV \
      --schema=producto:STRING,presentacion:STRING,marca:STRING,categoria:STRING,catalogo:STRING,precio:STRING,fechaRegistro:STRING,cadenaComercial:STRING,giro:STRING,nombreComercial:STRING,direccion:STRING,estado:STRING,municipio:STRING,latitud:STRING,longitud:STRING \
      mydataset_jbeas.testtable_schema \
      gs://de-bootcamp-2021/all_data.csv

```

Will ingest 18.71 GB of data into BQ table `testtable_schema`

Note large amount of ingested data.

## Querying from BigQuery

While querying, amount of processed data matters, since team will be charged based on those metrics.

```
select distinct(marca) from  `data-castle-bravo.mydataset_jbeas.testtable`;
select distinct(marca) from  `data-castle-bravo.mydataset_jbeas.testtable_schema`;
select marca from  `data-castle-bravo.mydataset_jbeas.testtable`;
select marca from  `data-castle-bravo.mydataset_jbeas.testtable_schema`;
```

will process 622.5 MiB when run.

and ... 
```
select precio from  `data-castle-bravo.mydataset_jbeas.testtable`;
```

will process 477.1 MiB when run.

and 
```
select precio from  `data-castle-bravo.mydataset_jbeas.testtable_schema`;
```

will process 338.5 MiB when run.

and ...

```
select marca, precio from  `data-castle-bravo.mydataset_jbeas.testtable`;
```

will process 1.1 GB when run, meaning that each field involved on the query adds bytes to procesing size.


but... please dont do this
```
select * from  `data-castle-bravo.mydataset_jbeas.testtable`;
select * from  `data-castle-bravo.mydataset_jbeas.testtable_schema`;
```

will process  the entire table 17.9 GB and 18.7 GB each


partitioned tables


```
bq load \
  --source_format=CSV \
  --max_bad_records=100 \
  --schema=producto:STRING,presentacion:STRING,marca:STRING,categoria:STRING,catalogo:STRING,precio:STRING,fechaRegistro:TIMESTAMP,cadenaComercial:STRING,giro:STRING,nombreComercial:STRING,direccion:STRING,estado:STRING,municipio:STRING,latitud:STRING,longitud:STRING \
  --time_partitioning_field fechaRegistro \
  --time_partitioning_type DAY \
  --require_partition_filter=false \
  de_app_dataset_jbeas.testtable_partitioned \
  gs://de-bootcamp-2021/all_data.csv
```


so...

```
select marca, precio from  `data-castle-bravo.mydataset_jbeas.testtable` where fechaRegistro > '2015-07-09 00:00:00 UTC';
```

will process 1.5 GiB when run and 

```
select marca, precio from  `data-castle-bravo.mydataset_jbeas.testtable_partitiones` where fechaRegistro > '2015-07-09 00:00:00 UTC';
```

will process 336.3 MiB when run

both queries will produce the same output.

## Creating tables from query's

simple table
```
CREATE TABLE `data-castle-bravo.mydataset_jbeas.marcas`
AS SELECT 
  ROW_NUMBER() OVER(order by marca) AS estado_id, marca
  FROM `data-castle-bravo.mydataset_jbeas.testtable_partitioned` 
  where marca <> ''
  group by marca 
  order by marca asc;
```
this table will store all the disctinct marcas wittin and id ordered by marca on descending order
```
CREATE TABLE `data-castle-bravo.mydataset_jbeas.estados`
AS 
SELECT 
  ROW_NUMBER() OVER(order by estado) AS estado_id, estado
  FROM `data-castle-bravo.mydataset_jbeas.testtable_partitioned` 
  where estado <> ''
  group by estado 
  order by estado asc;
```

Partitioned table with expiration date
```
CREATE TABLE `data-castle-bravo.mydataset_jbeas.marcas_precios`
 (
   fechaRegistro TIMESTAMP,
   marca STRING,
   precio STRING
 )
 PARTITION BY DATE(fechaRegistro)
 OPTIONS (
   partition_expiration_days=3,
   description="a table clustered by fechaRegistro"
 )
 AS SELECT fechaRegistro, marca, precio from `data-castle-bravo.mydataset_jbeas.testtable_partitioned`;
 ```
this table will split the data by fechaRegistro and will keep results only for 3 days

BigQuery can perform any regular sql query plus another extra features so feel free to perform any query as usual such as joins, group, order, distinct and so on.

## Sugested queries
### find the top 3 expensive products by estate
```
select 
  rank,
    estado,
    marca, 
    producto, 
    precio
from 
(
  select distinct
    RANK() OVER (PARTITION BY estado order by precio desc) as rank,
    estado,
    marca, 
    producto, 
    precio
  from `data-castle-bravo.mydataset_jbeas.testtable_partitioned`
  where estado <> ''
) as top_marcas
where (rank <= 3)
order by estado
```

### find the top 10 marcas having estado_id = 14 as a input parameter
```
SELECT 
  a.marca, a.estado, sum(1) as available_products
  FROM `data-castle-bravo.mydataset_jbeas.testtable_partitioned` as a
  inner join `data-castle-bravo.mydataset_jbeas.estados` as b on 
  a.estado = b.estado
  where b.estado_id = 14
  group by a.marca, a.estado 
  order by available_products desc
  limit 10;
```

# Querying from Redshift

While querying on Redshift, amount of data processed in unknown till the query is being executed.

In order to check real amount of processed data, you need to check into query details, from redshift main page. 

Runtime may vary depending on cluster size, table size and available worker nodes and memory avaiable.

```
select * from my_table limit 1;
```

in this example, even if this is a simple query, took 7 seconds to get results back, and 37 rows where scanned  making a total of 5.85 KB of scanned data.
same amount of time could be taken to get biger amount of data processed, it depends directly on cluster size and how table is structored.

Like in Bigquery, almost all standart SQL commands are available on redshift too plus another extra unique functions.

you can check translation between BigQuery and Redshit supported SQL here:

https://cloud.google.com/architecture/dw2bq/redshift/redshift-bq-sql-translation-reference.pdf

***
## General Tips

- Never perform a query without a where clause, its expensive on resources and money
- Try to avoid select *, select only the fields that are required for the task
- Use table preview as frecuent as possible instead of performing a query to check data types
- check table schema from schema tab instead of performing a query (show create table)
- while querying, try to put partitions first then other fields on where clause
- try to have at least one partition on where clause, when partitios avaiable
- if a query is being executed frecuently, try adding them into a view or temp table
